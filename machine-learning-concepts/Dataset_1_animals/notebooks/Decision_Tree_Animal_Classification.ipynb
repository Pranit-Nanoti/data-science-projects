{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification and Regression Trees (CART)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prani\\AppData\\Local\\Temp\\ipykernel_19932\\841749719.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "# Imports:\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ML Challenges - Model Underfitting vs Overfitting:\n",
    "\n",
    "There are two main <font color=#14F278>**Machine Learning Challenges**</font>:\n",
    "- **\"Bad\" Data**:\n",
    "    - insufficient quantity of training data\n",
    "    - non-representative training set\n",
    "    - poor quality of data\n",
    "    - irrelevant features/predictors\n",
    "- **\"Bad\" Model**:\n",
    "    - <font color=#14F278>**Overfitting**</font> - when the model <font color=#14F278>**'memorises'**</font> the training data and <font color=#14F278>**fails to generalise**</font> on unseen data\n",
    "    - <font color=#14F278>**Underfitting**</font> - when the model is <font color=#14F278>**too simple**</font> and <font color=#14F278>**fails to learn**</font> the underlying logic, structure and intricacies of the data\n",
    "\n",
    "---\n",
    "### Model Bias and Variance:\n",
    "\n",
    "The <font color=#14F278>**Bias of a Model**</font> measures how different the model predictions are, on average, compared to the true values on a given dataset.\n",
    "\\\n",
    "The <font color=#14F278>**Variance of a Model**</font> measures how sensitive the model predictions are to small variations in the input data:\n",
    "- Suppose we have an observation with a known true target value:\n",
    "- Suppose we take the model and train it multiple times, each time on different training set\n",
    "- Each trained model produces a prediction for the target value\n",
    "- We can <font color=#14F278>**compare the predictions to the true value**</font> to see how far off they are from the actual label - this measures the <font color=#14F278>**model bias**</font>\n",
    "- We can also <font color=#14F278>**compare the predictions to one another**</font> to see how clustered or dispersed they are from one another - this measures the <font color=#14F278>**model variance**</font>\n",
    "- A model with <font color=#14F278>**high bias**</font> indicates <font color=#14F278>**Underfitting**</font> - the model predictions are far away from the true value - the model fails to correctly capture the relationship between features and label\n",
    "- A model with <font color=#14F278>**high variance**</font> indicates <font color=#14F278>**Overfitting**</font> - the model predictions are way too sensitive to changes in the training data - the model thus fails to generalise - a slight change to the training data can lead to a significant difference in the predicted value\n",
    "- <font color=#FF8181>**Note: There is a trade-off between Bias and Variance!**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Decision Trees:\n",
    "\n",
    "A <font color=#14F278>**Decision Tree**</font> is a <font color=#14F278>**non-linear model**</font> where the input data is iteratively split into branches in a top-down tree-like structure, <font color=#14F278>**based on some conditions**</font>. **Decision Trees** can be applied both to <font color=#14F278>**Regressions**</font> and <font color=#14F278>**Classifications**</font>. \n",
    "\n",
    "---\n",
    "### CART - Classification and Regression Trees:\n",
    "<font color=#14F278>**Classification and Regression Trees (CART)**</font> work by <font color=#14F278>**partitioning**</font> the feature space into <font color=#14F278>**disjoint regions**</font>, based on a <font color=#14F278>**binary condition**</font>. Each region is then assigned a <font color=#14F278>**constant prediction**</font>:\n",
    "- Suppose we have 2 features in our feature space - $x_{1}$ and $x_{2}$:\n",
    "- We start splitting the feature space based on some binary conditions - e.g.,  $x_{1}<=5$, or $x_{2}>2$\n",
    "- The optimal split will result in a number of disjoin regions - here, there are 5 regions\n",
    "- Once the optimal split is produced, each region is associated with a <font color=#14F278>**constant prediction**</font> - e.g. region $R_{m}$ is assigned prediction value $c_{m}$ \n",
    "\n",
    "<center>\n",
    "    <div>\n",
    "        <img src=\"machine-learning-concepts/Dataset_1_animals/images/CART_split.JPG\"/>\n",
    "    </div>\n",
    "</center>\n",
    "\n",
    "Some terminology:\n",
    "- <font color=#14F278>**Root Node**</font>: NO *parent* node, condition results in 2 *children* nodes\n",
    "- <font color=#14F278>**Internal Node**</font>: one *parent* node, condition results in 2 *children* nodes\n",
    "- <font color=#14F278>**Leaf Node**</font>: one *parent* node, NO *children* nodes - associated with a *prediction*\n",
    "\n",
    "\n",
    "As the image suggests, <font color=#14F278>**CART**</font> work by splitting the *feature space* into disjoin regions, based on some rules - e.g. $x_{1}<=5$, or $x_{2}>2$. Once the most optimal split is performed, all data points in each region are assigned a **constant prediction** - e.g. region $R_{m}$ is assigned prediction value $c_{m}$:\n",
    "\n",
    "\n",
    "---\n",
    "### Regression Trees - How Do They Work?\n",
    "\n",
    "<center>\n",
    "    <div>\n",
    "        <img src=\"machine-learning-concepts/Dataset_1_animals/images/Regression_Trees.JPG\"/>\n",
    "    </div>\n",
    "</center>\n",
    "\n",
    "- We start with our **training data**, which consists of features and **known real-valued labels**\n",
    "- We perform an <font color=#14F278>**Optimal Split**</font> of the feature space, which defines the <font color=#14F278>**disjoint regions**</font>\n",
    "- Each region $R_{m}$ is assigned a constant prediction $c_{m}=\\frac{1}{N_{m}}\\sum_{i| x_{i} \\in R_{m}}y_{i}$\n",
    "- Intuitively, the prediction $c_{m}$ for region $R_{m}$ is the <font color=#14F278>**average of the labels for all training set observations in that region**</font>\n",
    "- When the model is used on a new unseen observation (test data), the observation is placed within one of the regions (based on its features), and assigned the constant prediction of that region\n",
    "\n",
    "---\n",
    "### Classification Trees - How Do They Work?\n",
    "\n",
    "<center>\n",
    "    <div>\n",
    "        <img src=\"machine-learning-concepts/Dataset_1_animals/images/Classification_Trees.JPG\"/>\n",
    "    </div>\n",
    "</center>\n",
    "\n",
    "- We start with our **training data**, which consists of features and **known categorical labels**\n",
    "- We perform an <font color=#14F278>**Optimal Split**</font> of the feature space, which defines the <font color=#14F278>**disjoint regions**</font>\n",
    "- Each region $R_{m}$ is assigned a constant prediction $c_{m}$ which is the <font color=#14F278>**dominating class in the region**</font>\n",
    "- When the model is used on a new unseen observation (test data), the observation is placed within one of the regions (based on its features), and assigned the constant prediction (class) of that region\n",
    "\n",
    "---\n",
    "### Recursive Binary Splitting:\n",
    "\n",
    "**Recursive Binary Splitting** is a **top-down, greedy algorithm**, used to grow Decision Trees:\n",
    "- The algorithm is performed iteratively on the tree\n",
    "- On each iteration, the algorithm solves for 2 components - <font color=#14F278>**the optimal feature**</font>, on which to perform the split, and the <font color=#14F278>**optimal feature cut-off value**</font> for the split - e.g., on the first split, the optimal feature to split by is $x_{1}$ and the cut-off value for the split is $x_{1} =5$\n",
    "- Recursive Binary Splitting is <font color=#14F278>**greedy**</font> as it is <font color=#14F278>**not forward-looking**</font> - each split is optimised in isolation to the splits that come after (further down the tree)\n",
    "- Each split results in <font color=#14F278>**two regions**</font>, associated with a constant prediction\n",
    "- For <font color=#14F278>**Regression problems**</font>, the optimal split results in the <font color=#14F278>**Lowest Sum of Squared Losses**</font> across the two regions\n",
    "- For <font color=#14F278>**Classification problems**</font>, the optimal split results in the <font color=#14F278>**Highest Information Gain**</font> across the two regions \n",
    "\n",
    "---\n",
    "#### Cross Entropy and Gini Index:\n",
    "**Recursive Binary Splitting** can involve calculating the **Information Gain** of a split for Classification:\n",
    "- Suppose that $\\hat{p_{mk}}$ is the <font color=#14F278>**proportion of observations**</font> in region $R_{m}$ from class $k$:\n",
    "- The split with the <font color=#14F278>**highest information gain**</font> is the split that results in the <font color=#14F278>**lowest node impurity**</font> in the two regions of the split\n",
    "- <font color=#14F278>**Node Impurity**</font> is measured via one of the below metrics:\n",
    "\n",
    "$$\n",
    "Cross Entropy = - \\sum_{k=1}^{K}\\hat{p_{mk}}log(\\hat{p_{mk}})\n",
    "$$\n",
    "\n",
    "$$\n",
    "Gini Index = \\sum_{k=1}^{K}\\hat{p_{mk}}(1-\\hat{p_{mk}})\n",
    "$$\n",
    "\n",
    "- The two measures produce similar results so let's focus on the <font color=#14F278>**Gini Index**</font>\n",
    "- The <font color=#14F278>**Gini Index**</font> intuitively measures the <font color=#14F278>**purity of classification**</font> in each region\n",
    "- For 2-class problem, it varies from **0 to 0.5** with **0** indicating perfect node purity (all observations belong to the same class) and **0.5** indicating dead-heath (observations are equally distributed across classes)\n",
    "- <font color=#14F278>**To maximise information gain, we want to minimise node impurity, i.e., we want the split to result in regions with a clear dominating class**</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# How Gini Index Measures Node Impurity\n",
    "# Example for 2 Classes: \n",
    "    # Class 1 with proportion P1\n",
    "    # vs\n",
    "    # Class 2 with proportion P2 = 1 - P1\n",
    "\n",
    "# Create a range of possible values for P1 - it ranges from 0 to 1\n",
    "p1 = np.linspace(0,1, 100)\n",
    "\n",
    "# Define a Gini Index Function\n",
    "def gini(x):\n",
    "    return 2*x*(1-x)\n",
    "\n",
    "# Calculate the Gini Index for each possible pair of P1 and P2 values\n",
    "gini_index = []\n",
    "for i in p1:\n",
    "    gini_index.append(gini(i))\n",
    "\n",
    "# Plot the Gini Index against P1 to see how Node Impurity peaks when we have 50-50% proportion across classes\n",
    "g = sns.lineplot(x = p1, y= gini_index, color = '#67B587')\n",
    "g.set_title('Gini Index for 2 Classes')\n",
    "g.set(xlabel = 'Proportion of Class 1', ylabel = 'Regional Gini Index')\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Decision Tree - Animal Classification:\n",
    "Using a **Decision Tree** to perform **Classification** on the Animal dataset, stored in `animals.csv` in the **data** folder:\n",
    "- The dataset containts 144 observations across different types of animals\n",
    "- Each observation has information such as **hair, feathers, legs, milk**, etc. which will serve as **features**\n",
    "- Each observation has a **class type** which is the target we want to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "filepath = r'../data/animals.csv'\n",
    "df = pd.read_csv(filepath)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of Class Type of the animal:\n",
    "\n",
    "df['class_type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Plotting a Decision Tree\n",
    "\n",
    "- To train a Decision Tree, use the `DecisionTreeClassifier()` class from `sklearn.tree` (Note - for Regression, use the `DecisionTreeRegressor()` class)\n",
    "- Perform a train-test split on the data with <font color=#14F278>**stratified sampling**</font> - this is because we currenly have an <font color=#14F278>**imbalanced class representation**</font> - 41 mammal observations, 20 fish, etc. Stratification will preserve the original proportions in the test and train data (to the extent to which it is possible)\n",
    "- Create a `DecisionTreeClassifier()` model with a <font color=#14F278>**maximum depth**</font> of 3 - maximum depth is a <font color=#14F278>**Hyperparameter**</font>, which controls how much the tree grows.\n",
    "- Fit the model to the training data\n",
    "- <font color=#14F278>**Plot the Decision Tree**</font> via the `plot_tree()` function from `sklearn.tree` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test Split and Feature Selection\n",
    "X = df.drop(columns = ['animal_name', 'class_type'])\n",
    "y = df['class_type']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=123, stratify=y)\n",
    "\n",
    "# Create a Decision Tree Classifier Model and fit to train set\n",
    "model = DecisionTreeClassifier(max_depth=3)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To visualise the tree, use the plot_tree function\n",
    "fig, ax = plt.subplots(figsize = (16,16))\n",
    "plot_tree(model, feature_names = model.feature_names_in_, class_names = model.classes_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, test model by making predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Visualise Confusion Matrix to assess how many false predictions your model made\n",
    "fig, ax = plt.subplots(figsize = (12,12))\n",
    "g=ConfusionMatrixDisplay.from_predictions( y_test, y_pred, ax = ax, cmap = 'BuGn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "---\n",
    "\n",
    "**Decision Trees** are Machine Learning Models with \n",
    "**Hyperparameters** - they are like tuning knobs for the model, to which we assign values before training and evaluating the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Hyperparameters - Definition:\n",
    "\n",
    "A **Hyperparameter** is a parameter, whose value is used to <font color=#14F278>**control the learning process of a model**</font>. Hyperparameters are like tuning knobs for a model - we set them to a particular value in advance, then train and evaluate the model.\n",
    "- For example, <font color=#14F278>**KNN**</font> has 2 hyperparameters - <font color=#14F278>**K - number of neighbours**</font>, and <font color=#14F278>**p - distance norm**</font>.\n",
    "- Decision Trees have a number of hyperparameters, one of which is  <font color=#14F278>**maximum depth**</font>, controling how deep the tree grows  \n",
    "- Hyperparameters are ultimately used to ensure that the model is a good fit for the task - a good choice of Hyperparameters allows us to prevent ML Challenges such as **Overfitting** or **Underfitting a model**\n",
    "\n",
    "\n",
    "---\n",
    "### Decision Trees - Hyperparameters:\n",
    "**Decision Trees are prone to Overfitting**. If we keep applying **Recursive Binary Splitting** to a tree, eventually, each data point may **end up in its own leaf node/region**. This means that the model becomes too sensitive to little changes in the input data, resulting in a **High Variance** - hence overfitting the model. \n",
    "\n",
    "We can **control the complexity** of the tree via the following Hyperparameters:\n",
    "\n",
    "<center>\n",
    "    <div>\n",
    "        <img src=\"machine-learning-concepts/Dataset_1_animals/images/Hyperparameter_Def.JPG\"/>\n",
    "    </div>\n",
    "</center>\n",
    "\n",
    "In general, <font color=#FF8181>**decreasing the maximum**</font> of a hyperparameter, such as `max_depth`, or <font color=#FF8181>**increasing the minimum**</font> of a hyperparameter, such as `min_samples_leaf`, will <font color=#FF8181>**reduce the complexity of the model, hence the model variance**</font> and ultimately <font color=#FF8181>**prevent from Overfitting!**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Hyperparameter Tuning:\n",
    "---\n",
    "### Validation Sets:\n",
    "- Suppose we want to find the **best hyperparameters** for a particular ML problem.\n",
    "- So far, we looked at splitting our data into **train and test sets**.\n",
    "- If we use the training set to **'tune' hyperparameters** over and over again, and use the **test set** to evaluate which hyperparameter values perform best, we are essentially **using the test set as part of the model training process**!\n",
    "- This is undesirable, because the purpose of test data is to **emulate** testing the model on unseen data.\n",
    "- Instead, we can split our data into **three sets:**\n",
    "    - **Train**\n",
    "    - **Validation**\n",
    "    - **Test**\n",
    "\n",
    "---\n",
    "### K-Fold Cross Validation:\n",
    "\n",
    "- Instead of splitting the original training set into a single train and validation set, we can perform **a number of splits**:\n",
    "- Split the train set into <font color=#14F278>**K non-overlapping subsets of data**</font> - also called **folds**\n",
    "- Take <font color=#14F278>**k-1 folds**</font> to train your model on and use the remaining fold for **validation**\n",
    "- Repeat the above step <font color=#14F278>**k times**</font>, each time, leaving a new fold out for validation, and using the rest for training\n",
    "- At the end, **average the performance estimates** across **all k validation folds** for which typical values for k are **k=5** or **k=10**\n",
    "- **Note:** K-Fold CV is used for Hyperparameter tuning - each model, trained with this approach, has different hyperpatameter values. The end goal of k-Fold CV is to find the model with the optimal hyperparameter values!\n",
    "\n",
    "<center>\n",
    "    <div>\n",
    "        <img src=\"machine-learning-concepts/Dataset_1_animals/images/Kfold_CV_Method.JPG\"/>\n",
    "    </div>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "### Grid Search CV:\n",
    "- **Question: How do we pick the possible combinations of hyperparameter values to run through the K-Fold CV?**\n",
    "- **Answer: We can use a technique, called Grid Search CV**\n",
    "\n",
    "**Grid Search CV** is a popular approach of exploring different combinations of hyperparameter values, by creating a **Hyperparameter Grid**:\n",
    "\n",
    "<center>\n",
    "    <div>\n",
    "        <img src=\"machine-learning-concepts/Dataset_1_animals/images/Hyperparameter_Grid.JPG\"/>\n",
    "    </div>\n",
    "</center>\n",
    "\n",
    "- **Grid Search CV** is implemented via the `GridSearchCV()` in `sklearn.model_selection`\n",
    "- We create an instance of the class, passing the following arguments:\n",
    "    - `estimator` - assigned to the instance of the model we want to tune the hyperparameters for\n",
    "    - `param_grid` - assigned to a dictionary of list values, where the keys are the **hyperparameter names** and the list values contain the ranges of values we want to search from\n",
    "    - `cv` - the number of folds, used for the **Cross Validation** - e.g., `cv=5`\n",
    "- The instance of the `GridSearchCV()` class implements `fit()` and `predict()` methods, which we use in the standard ways, by passing our train and test datasets\n",
    "\n",
    "---\n",
    "### Grid Search CV - Example (using animals.csv):\n",
    "- Training a **Decision Tree Classifier** and tuning its hyperparameters with **Grid Search CV**\n",
    "- The focus will be on two of the hyperparameters - `max_depth` and `min_samples_leaf`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "filepath = r'../data/animals.csv'\n",
    "df = pd.read_csv(filepath)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test Split and Feature Selection\n",
    "X = df.drop(columns = ['animal_name', 'class_type'])\n",
    "y = df['class_type']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=123, stratify=y)\n",
    "\n",
    "# Initialise Decision Tree Classifier\n",
    "model = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import GridSearch CV from sklearn\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# Pass Ranges for Hyperparameters\n",
    "parameters = {'max_depth':        [2,3,4,5,6,7,8,9,10],\n",
    "              'min_samples_leaf': [5, 10, 20]}  \n",
    "\n",
    "# Initialise a Grid Search object\n",
    "# passing the decision tree estimator and the parameter grid values\n",
    "clf = GridSearchCV(estimator = model, param_grid = parameters, cv=5)\n",
    "\n",
    "# Fit the Grid Search object to the train data\n",
    "# At this stage, the model is learning the optimal hyperparameter values by performing the Grid Search\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Once the model's hyperparameters are tuned, we can make predictions\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Apart from predicting, once the model is fit to the data, we can also **Optimal Hyperparameter Values**, found by the Grid Search CV\n",
    "- We can use the `best_estimator_.max_depth` or `best_estimator_.min_samples_leaf` attributes to find the optimal combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the optimal hyperparamer values, found by Grid Search CV\n",
    "best_depth = clf.best_estimator_.max_depth\n",
    "best_min_samples = clf.best_estimator_.min_samples_leaf\n",
    "\n",
    "print(f'best estimator: depth {best_depth} and min_samples {best_min_samples}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lastly, proceed with the final model evaluation by using the test set\n",
    "- We can plot the **Confusion Matrix** or calculate the **Accuracy Score** of the model\n",
    "- Recall that, above,  we trained a Decision Tree Classifier with `max_depth=3`\n",
    "- This time, we tuned the Hyperparameters of the same model with `GridSearchCV`\n",
    "- Below is a quick example comparison between the Confusion Matrix <font color=#14F278>**before and after Hyperparameter Tuning:**</font>\n",
    "\n",
    "<center>\n",
    "    <div>\n",
    "        <img src=\"machine-learning-concepts/Dataset_1_animals/images/Hyperparameter_Tuning_Effects.JPG\"/>\n",
    "    </div>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_score = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy score is {acc_score}')\n",
    "\n",
    "# Visualise Confusion Matrix to assess how many false predictions your model made\n",
    "fig, ax = plt.subplots(figsize = (12,12))\n",
    "g=ConfusionMatrixDisplay.from_predictions( y_test, y_pred, ax = ax, cmap = 'BuGn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lastly, we can directly train a `DecisionTreeClassifier` with the optimal values for the hyperparameters\n",
    "- Plot the Decision Tree with `plot_tree` to see how the hyperparameter values controlled the depth of the tree and the number of samples in each leaf node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise Decision Tree Classifier\n",
    "model = DecisionTreeClassifier(max_depth = 6, min_samples_leaf=5 )\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "acc_score = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy score is {acc_score}')\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (16,16))\n",
    "plot_tree(model, feature_names = model.feature_names_in_, class_names = model.classes_)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
