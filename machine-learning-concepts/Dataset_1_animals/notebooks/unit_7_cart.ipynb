{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=#14F278>Unit 7 - Classification and Regression Trees (CART)</font>\n",
    "---\n",
    "\n",
    "- In the previous units, we learnt the basics of ML programming and data pre-processing in the form of <font color=#14F278>**Fit-Predict**</font> and <font color=#14F278>**Fit-Transform Workflows**</font> and explored in detail <font color=#14F278>**Regression and Classification problems**</font>, first with <font color=#14F278>**KNN**</font> and then with <font color=#14F278>**Linear Models - OLS for Regression, Logistic Regression for Binary Classification**</font>.\n",
    "- In this and the next units, we will further expand our knowledge of algorithms for Regression and Classification in the form of <font color=#14F278>**Non-Linear Models**</font>\n",
    "- We will talk about <font color=#14F278>**Decision Trees (CART)**</font>, as well as <font color=#14F278>**Model Ensembles**</font>\n",
    "- Additionally, we will discuss a new feature of those models - <font color=#14F278>**Hyperparameters**</font>, and how to tune them to prevent <font color=#14F278>**Model Under- or Overfitting**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <font color=#14F278>1. ML Challenges - Model Underfitting vs Overfitting:</font>\n",
    "\n",
    "There are two main <font color=#14F278>**Machine Learning Challenges**</font>:\n",
    "- <font color=#14F278>**\"Bad\" Data:**</font>\n",
    "    - insufficient quantity of training data\n",
    "    - non-representative training set\n",
    "    - poor quality of data\n",
    "    - irrelevant features/predictors\n",
    "- <font color=#14F278>**\"Bad\" Model**</font>:\n",
    "    - <font color=#14F278>**Overfitting**</font> - when the model <font color=#14F278>**'memorises'**</font> the training data and <font color=#14F278>**fails to generalise**</font> on unseen data\n",
    "    - <font color=#14F278>**Underfitting**</font> - when the model is <font color=#14F278>**too simple**</font> and <font color=#14F278>**fails to learn**</font> the underlying logic, structure and intricacies of the data\n",
    "\n",
    "\n",
    "<center>\n",
    "    <div>\n",
    "        <img src=\"..\\images\\cart_001.png\"/>\n",
    "    </div>\n",
    "</center>\n",
    "\n",
    "---\n",
    "### <font color=#14F278>1.1 Model Bias and Variance:</font>\n",
    "\n",
    "- <font color=#FF8181>**Question: How do we identify a Model Overfit or Underfit?**</font>\n",
    "- <font color=#14F278>**Answer: By measuring the Model Bias and Variance**</font>\n",
    "\n",
    "The <font color=#14F278>**Bias of a Model**</font> measures how different the model predictions are, on average, compared to the true values on a given dataset.\n",
    "\\\n",
    "The <font color=#14F278>**Variance of a Model**</font> measures how sensitive the model predictions are to small variations in the input data:\n",
    "- Suppose we have an observation with a known true target value:\n",
    "- Suppose we take the model and train it multiple times, each time on different training set\n",
    "- Each trained model produces a prediction for the target value\n",
    "- We can <font color=#14F278>**compare the predictions to the true value**</font> to see how far off they are from the actual label - this measures the <font color=#14F278>**model bias**</font>\n",
    "- We can also <font color=#14F278>**compare the predictions to one another**</font> to see how clustered or dispersed they are from one another - this measures the <font color=#14F278>**model variance**</font>\n",
    "<center>\n",
    "    <div>\n",
    "        <img src=\"..\\images\\cart_002.PNG\"/>\n",
    "    </div>\n",
    "</center>\n",
    "\n",
    "\n",
    "- A model with <font color=#14F278>**high bias**</font> indicates <font color=#14F278>**Underfitting**</font> - the model predictions are far away from the true value - the model fails to correctly capture the relationship between features and label\n",
    "- A model with <font color=#14F278>**high variance**</font> indicates <font color=#14F278>**Overfitting**</font> - the model predictions are way too sensitive to changes in the training data - the model thus fails to generalise - a slight change to the training data can lead to a significant difference in the predicted value\n",
    "- <font color=#FF8181>**Note: There is a trade-off between Bias and Variance!**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <font color=#14F278>2. Decision Trees:</font>\n",
    "\n",
    "A <font color=#14F278>**Decision Tree**</font> is a <font color=#14F278>**non-linear model**</font> where the input data is iteratively split into branches in a top-down tree-like structure, <font color=#14F278>**based on some conditions**</font>. **Decision Trees** can be applied both to <font color=#14F278>**Regressions**</font> and <font color=#14F278>**Classifications**</font>. \n",
    "\n",
    "In essence, Decision Tree Models replicate the way we make conditional choices (decisions) in real-life:\n",
    "\n",
    "<center>\n",
    "    <div>\n",
    "        <img src=\"..\\images\\cart_003.PNG\"/>\n",
    "    </div>\n",
    "</center>\n",
    "\n",
    "---\n",
    "### <font color=#14F278>2.1 CART - Classification and Regression Trees:</font>\n",
    "<font color=#14F278>**Classification and Regression Trees (CART)**</font> work by <font color=#14F278>**partitioning**</font> the feature space into <font color=#14F278>**disjoint regions**</font>, based on a <font color=#14F278>**binary condition**</font>. Each region is then assigned a <font color=#14F278>**constant prediction**</font>:\n",
    "- Suppose we have 2 features in our feature space - $x_{1}$ and $x_{2}$:\n",
    "- We start splitting the feature space based on some binary conditions - e.g.,  $x_{1}<=5$, or $x_{2}>2$\n",
    "- The optimal split will result in a number of disjoin regions - here, there are 5 regions\n",
    "- Once the optimal split is produced, each region is associated with a <font color=#14F278>**constant prediction**</font> - e.g. region $R_{m}$ is assigned prediction value $c_{m}$ \n",
    "\n",
    "<center>\n",
    "    <div>\n",
    "        <img src=\"..\\images\\cart_004.PNG\"/>\n",
    "    </div>\n",
    "</center>\n",
    "\n",
    "Some terminology:\n",
    "- <font color=#14F278>**Root Node**</font>: NO *parent* node, condition results in 2 *children* nodes\n",
    "- <font color=#14F278>**Internal Node**</font>: one *parent* node, condition results in 2 *children* nodes\n",
    "- <font color=#14F278>**Leaf Node**</font>: one *parent* node, NO *children* nodes - associated with a *prediction*\n",
    "\n",
    "\n",
    "As the image suggests, <font color=#14F278>**CART**</font> work by splitting the *feature space* into disjoin regions, based on some rules - e.g. $x_{1}<=5$, or $x_{2}>2$. Once the most optimal split is performed, all data points in each region are assigned a **constant prediction** - e.g. region $R_{m}$ is assigned prediction value $c_{m}$:\n",
    "\n",
    "\n",
    "---\n",
    "### <font color=#14F278>2.2 Regression Trees - How Do They Work?</font>\n",
    "\n",
    "<center>\n",
    "    <div>\n",
    "        <img src=\"..\\images\\cart_005.PNG\"/>\n",
    "    </div>\n",
    "</center>\n",
    "\n",
    "- We start with our **training data**, which consists of features and **known real-valued labels**\n",
    "- We perform an <font color=#14F278>**Optimal Split**</font> of the feature space, which defines the <font color=#14F278>**disjoint regions**</font>\n",
    "- Each region $R_{m}$ is assigned a constant prediction $c_{m}=\\frac{1}{N_{m}}\\sum_{i| x_{i} \\in R_{m}}y_{i}$\n",
    "- Intuitively, the prediction $c_{m}$ for region $R_{m}$ is the <font color=#14F278>**average of the labels for all training set observations in that region**</font>\n",
    "- When the model is used on a new unseen observation (test data), the observation is placed within one of the regions (based on its features), and assigned the constant prediction of that region\n",
    "\n",
    "---\n",
    "### <font color=#14F278>2.3 Classification Trees - How Do They Work?</font>\n",
    "\n",
    "<center>\n",
    "    <div>\n",
    "        <img src=\"..\\images\\cart_006.PNG\"/>\n",
    "    </div>\n",
    "</center>\n",
    "\n",
    "- We start with our **training data**, which consists of features and **known categorical labels**\n",
    "- We perform an <font color=#14F278>**Optimal Split**</font> of the feature space, which defines the <font color=#14F278>**disjoint regions**</font>\n",
    "- Each region $R_{m}$ is assigned a constant prediction $c_{m}$ which is the <font color=#14F278>**dominating class in the region**</font>\n",
    "- When the model is used on a new unseen observation (test data), the observation is placed within one of the regions (based on its features), and assigned the constant prediction (class) of that region\n",
    "\n",
    "---\n",
    "### <font color=#14F278>2.4 Recursive Binary Splitting:</font>\n",
    "\n",
    "- <font color=#FF8181>**Question: How do we grow Decision Trees in the first place?**</font> In other words, how is <font color=#FF8181>**the optimal split found?**</font> \n",
    "- <font color=#14F278>**Answer:**</font> Via <font color=#14F278>**Recursive Binary Splitting**</font>\n",
    "\n",
    "<font color=#14F278>**Recursive Binary Splitting**</font> is a <font color=#14F278>**top-down, greedy algorithm**</font>, used to grow Decision Trees:\n",
    "- The algorithm is performed iteratively on the tree\n",
    "- On each iteration, the algorithm solves for 2 components - <font color=#14F278>**the optimal feature**</font>, on which to perform the split, and the <font color=#14F278>**optimal feature cut-off value**</font> for the split - e.g., on the first split, the optimal feature to split by is $x_{1}$ and the cut-off value for the split is $x_{1} =5$\n",
    "- Recursive Binary Splitting is <font color=#14F278>**greedy**</font> as it is <font color=#14F278>**not forward-looking**</font> - each split is optimised in isolation to the splits that come after (further down the tree)\n",
    "- Each split results in <font color=#14F278>**two regions**</font>, associated with a constant prediction\n",
    "- For <font color=#14F278>**Regression problems**</font>, the optimal split results in the <font color=#14F278>**Lowest Sum of Squared Losses**</font> across the two regions\n",
    "- For <font color=#14F278>**Classification problems**</font>, the optimal split results in the <font color=#14F278>**Highest Information Gain**</font> across the two regions \n",
    "\n",
    "---\n",
    "#### <font color=#14F278>2.4.1 Cross Entropy and Gini Index:</font>\n",
    "<font color=#14F278>**Recursive Binary Splitting**</font> can be mathematically involved, so we won't go into too much detail, however, it's worth mentioning what constitutes <font color=#14F278>**Information Gain**</font> of a split for Classification:\n",
    "- Suppose that $\\hat{p_{mk}}$ is the <font color=#14F278>**proportion of observations**</font> in region $R_{m}$ from class $k$:\n",
    "- The split with the <font color=#14F278>**highest information gain**</font> is the split that results in the <font color=#14F278>**lowest node impurity**</font> in the two regions of the split\n",
    "- <font color=#14F278>**Node Impurity**</font> is measured via one of the below metrics:\n",
    "\n",
    "$$\n",
    "Cross Entropy = - \\sum_{k=1}^{K}\\hat{p_{mk}}log(\\hat{p_{mk}})\n",
    "$$\n",
    "\n",
    "$$\n",
    "Gini Index = \\sum_{k=1}^{K}\\hat{p_{mk}}(1-\\hat{p_{mk}})\n",
    "$$\n",
    "\n",
    "- The two measures produce similar results so let's focus on the <font color=#14F278>**Gini Index**</font>\n",
    "- The <font color=#14F278>**Gini Index**</font> intuitively measures the <font color=#14F278>**purity of classification**</font> in each region\n",
    "- For 2-class problem, it varies from **0 to 0.5** with **0** indicating perfect node purity (all observations belong to the same class) and **0.5** indicating dead-heath (observations are equally distributed across classes)\n",
    "- <font color=#14F278>**To maximise information gain, we want to minimise node impurity, i.e., we want the split to result in regions with a clear dominating class**</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualise How Gini Index Measures Node Impurity\n",
    "# Example for 2 Classes - Class 1 with proportion P1 and Class 2 with proportion P2 = 1 - P1\n",
    "\n",
    "# Create a range of possible values for P1 - it ranges from 0 to 1\n",
    "p1 = np.linspace(0,1, 100)\n",
    "\n",
    "# Define a Gini Index Function\n",
    "def gini(x):\n",
    "    return 2*x*(1-x)\n",
    "\n",
    "# Calculate gini index for each possible pair of P1 and P2 values\n",
    "gini_index = []\n",
    "for i in p1:\n",
    "    gini_index.append(gini(i))\n",
    "\n",
    "# Plot Gini Index against P1 to see how Node Impurity peaks when we have 50-50% proporion across classes\n",
    "g = sns.lineplot(x = p1, y= gini_index, color = '#67B587')\n",
    "g.set_title('Gini Index for 2 Classes')\n",
    "g.set(xlabel = 'Proportion of Class 1', ylabel = 'Regional Gini Index')\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <font color=#14F278>3. Decision Tree - Animal Classification:</font>\n",
    "Let's use a <font color=#14F278>**Decision Tree**</font> to perform <font color=#14F278>**Classification**</font> on the Animal dataset, stored in `animals.csv` in the **data** folder\n",
    "- The dataset containts 144 observations across different types of animals\n",
    "- Each observation has information such as **hair, feathers, legs, milk**, etc. which will serve as **features**\n",
    "- Each observation has a **class type** which is the target we want to predict\n",
    "- Explore the data to familiarise yourself with the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "filepath = r'../data/animals.csv'\n",
    "df = pd.read_csv(filepath)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['class_type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To train a Decision Tree, use the `DecisionTreeClassifier()` class from `sklearn.tree` (Note - for Regression, use the `DecisionTreeRegressor()` class)\n",
    "- Perform a train-test split on the data with <font color=#14F278>**stratified sampling**</font> - this is because we currenly have an <font color=#14F278>**imbalanced class representation**</font> - 41 mammal observations, 20 fish, etc. Stratification will preserve the original proportions in the test and train data (to the extent to which it is possible)\n",
    "- Create a `DecisionTreeClassifier()` model with a <font color=#14F278>**maximum depth**</font> of 3 - maximum depth is a <font color=#14F278>**Hyperparameter**</font>, which controls how much the tree grows. More on Hyperparameters later\n",
    "- Fit the model to the training data\n",
    "- <font color=#14F278>**Plot the Decision Tree**</font> via the `plot_tree()` function from `sklearn.tree` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test Split and Feature Selection\n",
    "X = df.drop(columns = ['animal_name', 'class_type'])\n",
    "y = df['class_type']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=123, stratify=y)\n",
    "\n",
    "# Create a Decision Tree Classifier Model and fit to train set\n",
    "model = DecisionTreeClassifier(max_depth=3)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To visualise the tree, use the plot_tree function\n",
    "fig, ax = plt.subplots(figsize = (16,16))\n",
    "plot_tree(model, feature_names = model.feature_names_in_, class_names = model.classes_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, test model by making predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Visualise Confusion Matrix to assess how many false predictions your model made\n",
    "fig, ax = plt.subplots(figsize = (12,12))\n",
    "g=ConfusionMatrixDisplay.from_predictions( y_test, y_pred, ax = ax, cmap = 'BuGn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### <font color=#14F278>3.1 Decision Trees - Notes on Pre-Processing:</font>\n",
    "<font color=#14F278>**Decision Trees**</font> require <font color=#14F278>**much less Data-Preprocessing**</font> than Linear Models, which makes them very convenient to use:\n",
    "- CART don't require <font color=#14F278>**numerical data transformations**</font> such as Normalisation or Standardisation as they are insensitive to the scale of data, unlike linear models, or any models, whose prediction is based on a distance function\n",
    "- CART doesn't require <font color=#14F278>**One-Hot Encoding**</font> for nominal categorical features\n",
    "- However, `sklearn` requires implementations of CART to work with <font color=#14F278>**categorical values converted to numerical**</font>, so if you have categorical features of any kind, <font color=#14F278>**perform Ordinal Encoding on them first**</font>\n",
    "- CART doesn't require the <font color=#14F278>**target labels**</font> to be numerical - you can go ahead with string labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <font color=#14F278>4. Decision Trees vs Linear Models:</font>\n",
    "- Linear Models outperform Decision Trees, when the <font color=#14F278>**true relationship between features and target**</font> is <font color=#14F278>**linear**</font>, and vice-versa\n",
    "- Consider the following simple Classification illustration where we have 2 features $x_{1}$ and $x_{2}$ and the class is visualised via the **colour**:\n",
    "\n",
    "<center>\n",
    "    <div>\n",
    "        <img src=\"..\\images\\cart_007.PNG\"/>\n",
    "    </div>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <font color=#FF8181>5. Summary:</font>\n",
    "- **Overfitting** is when a model **memorises** the training data and **fails to generalise** on unseen data. Model Overfit associates with **high Model Variance**\n",
    "- **Underfitting** is when a model is **too simple** to learn the underlying logic, structure and intricacies of the training data. Model Underfit associates with **high Model Bias**\n",
    "- **Classification and Regression Trees (CART)** are **non-linear models**, used for both **Classification** and **Regression**, where the input data is iteratively split into branches in a top-down tree-like structure\n",
    "- With Decision Trees, the feature space is separated into **disjoint regions**, each associated with a **constant prediction**\n",
    "- For **Regression**, the constant prediction for a given region is the **average of the labels for all training data observations in the region**\n",
    "- For **Classification**, the constant prediction for a given region is the **most prevalent class**across all training data observations in the region\n",
    "- Decision Trees are grown via **Recursive Binary Splitting**, which is a top-down greedy algorithm, which on each step finds the optimal feature and cut-off value, based on which to perform the split\n",
    "- The optimal split for Regression problems is the split which minimises the **Sum of Squared Errors** for all training data points\n",
    "- The optimal split for Classification problems is the split which minimises the **Node Impurity** for each region, i.e., which results in the **highest information gain**\n",
    "- Decision Trees require **much-less data pre-processing** than Linear Models\n",
    "- Decision Trees perform better, when the **true relationship between feature and target is non-linear**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
